# -*- coding: utf-8 -*-
"""Model selection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y3xOiLv1UjuoVli_jwSjT1N4t-4GtmRi
"""

pip install rdkit

pip install pandas

pip install matplotlib

pip install scikit-learn

pip install torch-geometric

from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem as ch
from rdkit.Chem import Draw as d
from rdkit import DataStructs
import pandas as pd
from rdkit.Chem import rdMolDescriptors as rdescriptors
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import csv
from rdkit.SimDivFilters.rdSimDivPickers import MaxMinPicker
import sklearn
from rdkit.Chem import PandasTools, Descriptors, MolFromSmiles
from pandas import DataFrame
from sklearn.model_selection import train_test_split
from sklearn import svm
import numpy as np
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from rdkit import Chem
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import torch
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv

from google.colab import drive
drive.mount('/content/drive')

csv_path = '/content/drive/My Drive/bixuchenggong11.CSV'
df = pd.read_csv(csv_path, usecols=[1, 2, 3])
df.iloc[35:42]

with open ('/content/drive/My Drive/bixuchenggong11.CSV', 'r') as f:
    content_raw = list((csv.reader(f, delimiter = '\t')))
len(content_raw)

content=[]
for i in range(0,len(content_raw)):
    if i == 0:
        chembl_id=content_raw[i][0]
        content.append(content_raw[i])
    elif content_raw[i][0]!=chembl_id:
        chembl_id=content_raw[i][0]
        content.append(content_raw[i])

new_content = content[1:]
new_content = [''.join(sublist) for sublist in new_content]

import rdkit.Chem as ch
names = []
smiles = []
mols = []

for item in new_content:

    parts = item.split(',')

    names.append(parts[1].strip())
    smiles.append(parts[3].strip())
    mols.append(ch.MolFromSmiles(parts[3].strip()))

activity = []

for item in new_content:
    if item.strip():
        parts = item.split(',')
        try:
            activity.append(float(parts[2]))
        except ValueError:
            print("Error: Unable to convert string to float:", parts[2])

print("Activity:", activity)

import rdkit.Chem as Chem


def extract_hydrogen_bonds_count(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        hydrogen_bonds = mol.GetSubstructMatches(Chem.MolFromSmarts('[OH]'))
        if hydrogen_bonds:
            return len(hydrogen_bonds)
        else:
            return 0
    else:
        print("Invalid SMILES:", smiles)
        return None


hydrogen_bonds_counts = [extract_hydrogen_bonds_count(sm) for sm in smiles]


for i, count in enumerate(hydrogen_bonds_counts):
    print(f"SMILES {i+1}: Hydrogen bonds count = {count}")

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors

wiener_indices = []

for smile in smiles:
    mol = Chem.MolFromSmiles(smile)
    if mol:

        wiener_index = Descriptors.MolWt(mol)
        wiener_indices.append(wiener_index)
    else:

        print(f"Invalid SMILES: {smile}")
        wiener_indices.append(None)


dataframe_features = pd.DataFrame({
    'Name': names,
    'Activity': activity,
    'SMILES': smiles,
    'Wiener Index': wiener_indices
})


print(dataframe_features)

for index, value in enumerate(wiener_indices):
    print(f"Type of wiener_indices[{index}]: {type(value)}")

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors, rdmolops, rdMolDescriptors, GraphDescriptors
from sklearn.preprocessing import MinMaxScaler
from rdkit import Chem
from rdkit.Chem import AllChem, MACCSkeys
import numpy as np

import pandas as pd
from rdkit import Chem
from rdkit.Chem import MACCSkeys

from rdkit.Chem import GetSymmSSSR

from rdkit.Chem import GetSymmSSSR


def extract_ring_features(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None, None
    ring_counts = {'total_rings': Chem.rdMolDescriptors.CalcNumRings(mol)}
    num_rings = ring_counts['total_rings']
    for size in range(3, 7):
        try:
            ring_counts[f'ring_size_{size}'] = len(Chem.GetSymmSSSR(mol, maxPathLength=size, minToFind=size))
        except:
            ring_counts[f'ring_size_{size}'] = 0
    return ring_counts, num_rings


dataframe_features = pd.DataFrame(columns=['num_rings'])

for i, smiles_str in enumerate(smiles):
    features_dict, num_rings = extract_ring_features(smiles_str)
    if features_dict is not None:

        dataframe_features = pd.concat([dataframe_features, pd.DataFrame([features_dict])], ignore_index=True)




print(dataframe_features)




def extract_atom_count(mol, atom_symbols):
    atom_counts = {symbol: 0 for symbol in atom_symbols}
    for atom in mol.GetAtoms():
        symbol = atom.GetSymbol()
        if symbol in atom_counts:
            atom_counts[symbol] += 1
    return atom_counts




def extract_bond_count(mol, bond_type):
    bond_count = 0
    for bond in mol.GetBonds():
        if bond.GetBondType() == bond_type:
            bond_count += 1
    return bond_count


def extract_bond_type_count(mol, bond_type_str):
    bond_counts = {'single': 0, 'double': 0, 'triple': 0, 'aromatic': 0}
    for bond in mol.GetBonds():
        bond_type = str(bond.GetBondType()).lower()
        if bond_type == bond_type_str:
            bond_counts[bond_type] += 1
        elif bond.GetIsAromatic() and bond_type_str == 'aromatic':
            bond_counts['aromatic'] += 1
    return bond_counts[bond_type_str]
def extract_ring_features(mol):
    ring_counts = {'total_rings': Chem.rdMolDescriptors.CalcNumRings(mol)}
    for size in range(3, 11):
        try:
            ring_counts[f'ring_size_{size}'] = len(Chem.GetSymmSSSR(mol, maxPathLength=size, minToFind=size))
        except:
            ring_counts[f'ring_size_{size}'] = 0
    return ring_counts

def extract_topology_features(mol):
    topology_features = {}

    topology_features['balaban_j'] = GraphDescriptors.BalabanJ(mol)
    return topology_features




def extract_features(smiles):
    features_list = []
    for smiles in smiles:
        mol = Chem.MolFromSmiles(smiles)
        if mol:
            features = {}
            features['num_atoms'] = mol.GetNumAtoms()
            features['num_heavy_atoms'] = Descriptors.HeavyAtomCount(mol)
            features['num_bonds'] = mol.GetNumBonds()

            features['num_rotatable_bonds'] = Descriptors.NumRotatableBonds(mol)
            features['num_aromatic_rings'] = Descriptors.NumAromaticRings(mol)
            features['num_saturated_rings'] = Descriptors.NumSaturatedRings(mol)
            features['num_aliphatic_rings'] = Descriptors.NumAliphaticRings(mol)



            features['MW'] = Descriptors.MolWt(mol)
            features['logP'] = Descriptors.MolLogP(mol)
            features['TPSA'] = Descriptors.TPSA(mol)
            features['HBD'] = Descriptors.NumHDonors(mol)
            features['HBA'] = Descriptors.NumHAcceptors(mol)
            features['fraction_csp3'] = Descriptors.FractionCSP3(mol)
            features['num_saturated_carbocycles'] = Descriptors.NumSaturatedCarbocycles(mol)
            features['num_saturated_heterocycles'] = Descriptors.NumSaturatedHeterocycles(mol)
            features['num_aliphatic_heterocycles'] = Descriptors.NumAliphaticHeterocycles(mol)


            features['num_hydrogen_bonds'] = extract_hydrogen_bonds_count(smiles)



            features['single_bond'] = extract_bond_count(mol, Chem.BondType.SINGLE)
            features['double_bond'] = extract_bond_count(mol, Chem.BondType.DOUBLE)
            features['triple_bond'] = extract_bond_count(mol, Chem.BondType.TRIPLE)
            features['aromatic_bond'] = extract_bond_count(mol, Chem.BondType.AROMATIC)
            features['single'] = extract_bond_type_count(mol, 'single')
            features['aromatic'] = extract_bond_type_count(mol, 'aromatic')
            ring_feats = extract_ring_features(mol)
            for key, value in ring_feats.items():
                features[key] = value
            topo_feats = extract_topology_features(mol)
            for key, value in topo_feats.items():
                features[key] = value

            features_list.append(features)
        else:
            print("Invalid SMILES:", smiles)
    return pd.DataFrame(features_list)


dataframe_features = extract_features(smiles)
dataframe_features['Activity'] = activity
dataframe_features['SMILES'] = smiles

wiener_indices_float = [float(val) if val is not None else None for val in wiener_indices]

dataframe_features['Wiener Index'] = wiener_indices_float
dataframe_features['num_rings'] = num_rings
print(dataframe_features)

import pandas as pd

data = pd.read_csv('/content/drive/My Drive/shishikan.csv')
new_features = [
    'name',  'MW', 'Vol', 'Dense', 'nHA', 'nHD', 'TPSA', 'nRot',
    'nRing', 'MaxRing', 'nHet', 'fChar', 'nRig', 'Flex', 'nStereo', 'gasa', 'QED',
    'Synth', 'Fsp3', 'MCE-18', 'Natural Product-likeness', 'GSK', 'GoldenTriangle',
    'logS', 'logD', 'logP', 'mp', 'bp', 'pka_acidic', 'pka_basic', 'caco2', 'MDCK',
    'PAMPA', 'pgp_inh', 'pgp_sub', 'hia', 'f20', 'f30', 'f50', 'OATP1B1', 'OATP1B3',
    'BCRP', 'BSEP', 'BBB', 'MRP1', 'PPB', 'logVDss', 'Fu', 'CYP1A2-inh', 'CYP1A2-sub',
    'CYP2C19-inh', 'CYP2C19-sub', 'CYP2C9-inh', 'CYP2C9-sub', 'CYP2D6-inh', 'CYP2D6-sub',
    'CYP3A4-inh', 'CYP3A4-sub', 'CYP2B6-inh', 'CYP2B6-sub', 'CYP2C8-inh', 'LM-human',
    'cl-plasma', 't0.5', 'BCF', 'IGC50', 'LC50DM', 'LC50FM', 'hERG', 'SkinSen',
    'Respiratory', 'H-HT', 'Neurotoxicity-DI', 'Ototoxicity', 'Hematotoxicity',
    'Nephrotoxicity-DI', 'Genotoxicity', 'RPMI-8226', 'A549', 'HEK293', 'NR-AhR',
    'NR-AR', 'NR-AR-LBD', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',
    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53', 'Aggregators', 'Fluc',
    'Blue_fluorescence', 'Green_fluorescence', 'Reactive', 'Other_assay_interference',
    'Promiscuous'
]

dataframe_features = extract_features(smiles)


new_feature_data = data[new_features]


new_feature_data = new_feature_data.set_index(dataframe_features.index)


dataframe_features = pd.concat([dataframe_features, new_feature_data], axis=1)

dataframe_feature = dataframe_features.loc[:, (dataframe_features != 0).any()]

dataframe_features = dataframe_features.loc[:, ~dataframe_features.columns.duplicated()]

import numpy as np
import pandas as pd


dataframe_feature_numeric = dataframe_feature.apply(pd.to_numeric, errors='coerce')


dataframe_feature_numeric = dataframe_feature_numeric.dropna()


feature_variances = dataframe_feature_numeric.var()

threshold = np.percentile(feature_variances, 75)

print("Upper quartile as the threshold:", threshold)

from sklearn.feature_selection import VarianceThreshold
import pandas as pd


numeric_columns = dataframe_feature.select_dtypes(include=['float64', 'int64']).columns

selector = VarianceThreshold(threshold=0.028)
selected_features = selector.fit_transform(dataframe_feature[numeric_columns])

selected_indices = selector.get_support(indices=True)

print("Selected feature indices:", selected_indices)

from sklearn.feature_selection import VarianceThreshold
import pandas as pd


numeric_columns = dataframe_feature.select_dtypes(include=['float64', 'int64']).columns

selector = VarianceThreshold(threshold=0.027)
selected_features = selector.fit_transform(dataframe_feature[numeric_columns])


selected_indices = selector.get_support(indices=True)


selected_feature_names = dataframe_feature[numeric_columns].columns[selected_indices]

selected_features_df = pd.DataFrame(selected_features, columns=selected_feature_names)

selected_features_df_unique = selected_features_df.loc[:,~selected_features_df.columns.duplicated()]

print("Feature names after removing redundant features:", selected_features_df_unique.columns)
print(selected_features_df_unique.dtypes)

print("selected_feature_namesï¼š", selected_feature_names)
print("Feature data after removing redundant features:")
print(selected_features_df_unique)

from sklearn.ensemble import IsolationForest
import pandas as pd
import numpy as np


clf = IsolationForest(contamination=0.05, random_state=42)
outliers = clf.fit_predict(selected_features_df_unique)

selected_features_df_unique['outlier'] = outliers


outliers_df = selected_features_df_unique[selected_features_df_unique['outlier'] == -1]


print("Rows with outliers:")
print(outliers_df)


for col in selected_features_df_unique.columns:
    if col != 'outlier':

        col_mean = outliers_df[col].mean()

        selected_features_df_unique.loc[selected_features_df_unique['outlier'] == -1, col] = col_mean


selected_features_df_unique.drop('outlier', axis=1, inplace=True)

filled_features_df = selected_features_df_unique.copy()

print("Filled features dataset:")
print(filled_features_df)

dataframe_features = pd.DataFrame({
    'Activity': activity,
})

from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.model_selection import train_test_split
import pandas as pd


X = filled_features_df.values
y = dataframe_features['Activity'].values

estimator = SVC(kernel='linear')


rfe = RFE(estimator, n_features_to_select=12, step=1)


rfe.fit(X, y)


selected_feature_indexes = rfe.get_support(indices=True)
selected_features = filled_features_df.columns[selected_feature_indexes]

print("Selected Features using RFE with SVM:")
print(selected_features)


selected_features_df = filled_features_df[selected_features]
print("DataFrame with selected features based on RFE with SVM:")
print(selected_features_df)

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data['Feature'] = X.columns
    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

vif_result = calculate_vif(selected_features_df)

print("Variance Inflation Factors (VIF):")
print(vif_result)

selected_features_df = filled_features_df[['HBD', 'Natural Product-likeness', 'OATP1B1', 'CYP1A2-sub',
                                           'CYP2D6-inh', 'LM-human', 'LC50FM', 'Hematotoxicity',
                                           'SR-HSE', 'SR-MMP', 'Fluc', 'Promiscuous']]

vif_values = {
    'Feature': ['HBD', 'Natural Product-likeness', 'OATP1B1', 'CYP1A2-sub', 'CYP2D6-inh', 'LM-human',
                'LC50FM', 'Hematotoxicity', 'SR-HSE', 'SR-MMP', 'Fluc', 'Promiscuous'],
    'VIF': [4.673960, 5.846309, 7.645981, 28.253977, 19.747913, 9.727952, 79.092133,
            7.157007, 4.568013, 17.416982, 3.771040, 5.864861]
}


vif_df = pd.DataFrame(vif_values)


features_to_reduce = vif_df[vif_df['VIF'] > 5]['Feature'].tolist()
features_to_keep = vif_df[vif_df['VIF'] <= 5]['Feature'].tolist()


from sklearn.decomposition import PCA

pca = PCA(n_components=4)


reduced_features = pca.fit_transform(selected_features_df[features_to_reduce])


reduced_features_df = pd.DataFrame(data=reduced_features, columns=[f'PCA_Reduced_Feature_{i+1}' for i in range(pca.n_components)])


final_selected_features_df = pd.concat([reduced_features_df, selected_features_df[features_to_keep]], axis=1)


print("Final DataFrame with selected features based on RFE with PCA (Reduced to 4 dimensions):")
print(final_selected_features_df.head())

import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB


X = final_selected_features_df
y = dataframe_features['Activity']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=36)


X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)




models = {
    "Logistic Regression": LogisticRegression(random_state=36),
    "Support Vector Machine": SVC(probability=True, random_state=36),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gradient Boosting Classifier": GradientBoostingClassifier(random_state=36),
    "Multilayer Perceptron": MLPClassifier(random_state=36),
    "Naive Bayes": GaussianNB(),
}


for name, model in models.items():

    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')


    cv_std = np.std(cv_scores)


    print(f"{name}:")
    print(f"Cross-validation scores: {cv_scores}")
    print(f"Mean CV accuracy: {np.mean(cv_scores):.4f}")
    print(f"CV accuracy std: {cv_std:.4f}")
    print("="*40)


    model.fit(X_train, y_train)


    y_val_pred = model.predict(X_val)


    val_accuracy = accuracy_score(y_val, y_val_pred)
    val_report = classification_report(y_val, y_val_pred, output_dict=True)
    val_precision = val_report['weighted avg']['precision']
    val_recall = val_report['weighted avg']['recall']
    val_f1 = val_report['weighted avg']['f1-score']

    print(f"Validation Set Evaluation for {name}:")
    print(f"Accuracy: {val_accuracy:.4f}")
    print(f"Precision: {val_precision:.4f}")
    print(f"Recall: {val_recall:.4f}")
    print(f"F1 Score: {val_f1:.4f}")
    print("-"*40)


    y_test_pred = model.predict(X_test)


    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_report = classification_report(y_test, y_test_pred, output_dict=True)
    test_precision = test_report['weighted avg']['precision']
    test_recall = test_report['weighted avg']['recall']
    test_f1 = test_report['weighted avg']['f1-score']


    print(f"Test Set Evaluation for {name}:")
    print(f"Accuracy: {test_accuracy:.4f}")
    print(f"Precision: {test_precision:.4f}")
    print(f"Recall: {test_recall:.4f}")
    print(f"F1 Score: {test_f1:.4f}")
    print("="*40)

plt.figure(figsize=(10, 8))

for name, model in models.items():

    model.fit(X_train, y_train)


    plot_roc_curve(model, X_val, y_val, label=name)

dpi_value = 600
plt
save_path = '/content/example_plot1.svg'


dpi_value = 600
plt.savefig(save_path, dpi=dpi_value)

from google.colab import files
files.download(save_path)